{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.06 pset 6\n",
    "\n",
    "Due Wednesday October 17 at 10:55am."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (10 points)\n",
    "\n",
    "Recall that, if $x \\in \\mathbb{R}^n$, then $\\nabla_x f(x)$ (for a scalar-valued function $f$) is a column vector\n",
    "$$\n",
    "\\nabla_x f = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{pmatrix}\n",
    "$$\n",
    "(This is the \"uphill\" direction in which $f$ changes most rapidly.)\n",
    "\n",
    "**(a)** If $f(x) = \\frac{x^T A x}{x^T x}$ for some $n \\times n$ matrix $A$ (not necessarily symmetric!) and $x \\ne 0$, write $\\nabla_x f$ as a matrix expression (not individual components) involving $A$ and $x$.\n",
    "\n",
    "**(b)** For the $f(x)$ from (a), $f(\\alpha x)$ has what relationship to $f(x)$ for any real $\\alpha \\ne 0$?  It follows that $\\nabla_x f$ must be *orthogonal* to what vector?  Check that this is true of your answer from (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** We have $f(x) = \\frac{x^T A x}{x^T x} = (x^TAx)(x^Tx)^{-1}$. We can calculate the partial derivative with respect to $x_i$ using the chain rule, as in class, via $\\partial x/\\partial x_i = e_i$ where $e_i$ is the unit vector in direction $i$:\n",
    "\\begin{align}\n",
    "\\frac{\\partial f}{\\partial x_i} = (x^TAx)\\frac{\\partial}{\\partial x_i} \\left[(x^Tx)^{-1}\\right] + (x^Tx)^{-1}\\frac{\\partial}{\\partial x_i} (x^TAx)\n",
    "\\end{align}\n",
    "Now\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial x_i} (x^Tx)^{-1} &= -(x^Tx)^{-2} \\left[e_i^T x + x^Te_i\\right] = -\\frac{2x_i}{(x^Tx)^2}\\\\\n",
    "\\frac{\\partial}{\\partial x_i} (x^TAx) &= e_i^TAx + x^TAe_i\n",
    "\\end{align}\n",
    "and so we can put these two pieces together by noticing, for example, that $e_i^TAx$ is the ith component of the vector $Ax$, to yield: \n",
    "\\begin{align}\n",
    "\\boxed{\\nabla_x f = \\frac{(x^Tx)\\left[Ax + A^Tx\\right] - 2(x^TAx)x}{(x^Tx)^2}}\n",
    "\\end{align}\n",
    "If we want, we can simplify this in various ways.   One way to write it would be to substitute in the definition of $f(x)$, to obtain\n",
    "\\begin{align}\n",
    "\\boxed{\\nabla_x f = \\frac{\\left[A + A^T - 2f(x)I\\right]x}{x^Tx}}\n",
    "\\end{align}\n",
    "Another way is to realize that $2x^TAx = x^T Ax + x^T A^T x$ to pull out the $A+A^T$ factor, combined with $(x^TAx)x = xx^TAx$ so that we can pull out $xx^T$:\n",
    "\\begin{align}\n",
    "\\boxed{\\nabla_x f = \\left(I-\\frac{xx^T}{x^Tx}\\right)\\frac{(A+A^T)x}{x^T x}}\n",
    "\\end{align}\n",
    "This form is especially nice for part (b) because it contains the explicit orthogonal projection $I-\\frac{xx^T}{x^Tx}$ onto the line perpendicular to $x$.\n",
    "\n",
    "**(b)** If $f(x) = \\frac{x^T A x}{x^T x}$, then \n",
    "\\begin{align}\n",
    "f(\\alpha x) = \\frac{(\\alpha x)^TA(\\alpha x)}{(\\alpha x)^T(\\alpha x)} = \\frac{x^T A x}{x^T x} = f(x)\n",
    "\\end{align}\n",
    "This then tells us that $f(x)$ is constant along any given line through the origin. We know from 18.02 that the gradient of a function is always perpendicular to the level curves of a function, and so we expect that $\\boxed{x^T \\nabla_x f = 0}$.  We can easily verify this by plugging in one of our expressions for $\\nabla_x f$ above.\n",
    "\n",
    "From our third expression for $\\nabla_x f$, in fact, the $I-\\frac{xx^T}{x^Tx}$ factor tells us immediately that the gradient of $f$ is always orthogonal to $x$, because this is the orthogonal projection matrix onto $N(x^T)$.\n",
    "\n",
    "Alternatively, we can just plug in $x^T \\nabla_x f$.  For example, with our first expression above, we get:\n",
    "\\begin{align}\n",
    "x^T \\nabla f(x) &= \\frac{(x^Tx)\\left[x^TAx + x^TA^Tx\\right] - 2(x^TAx)x^Tx}{(x^Tx)^2}\\\\\n",
    "&= \\frac{(x^Tx)\\left[2x^TAx - 2(x^TAx)\\right]}{(x^Tx)^2}\\\\\n",
    "&= 0 \n",
    "\\end{align}\n",
    "where the second line follows from the fact that $x^TAx $ is a scalar and so $x^TAx = (x^TAx)^T = x^TA^T x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (5 points)\n",
    "\n",
    "If $f(A)$ is a scalar function of an $m\\times n$ *matrix* $A = \\begin{pmatrix} a_{11} & a_{12} & \\cdots \\\\ a_{21} & a_{22} & \\cdots \\\\ \\vdots & \\vdots & \\ddots \\end{pmatrix}$, then it is useful define the gradient with respect to the *matrix* as another $m\\times n$ matrix:\n",
    "$$\n",
    "\\nabla_A f = \\begin{pmatrix} \\frac{\\partial f}{\\partial a_{11}} & \\frac{\\partial f}{\\partial a_{12}} & \\cdots \\\\ \\frac{\\partial f}{\\partial a_{21}} & \\frac{\\partial f}{\\partial a_{22}} & \\cdots \\\\ \\vdots & \\vdots & \\ddots \\end{pmatrix}\n",
    "$$\n",
    "Given this definition, give a matrix expression (not in terms of individual components) for $\\nabla_A f$ with $f(A) = x^T A y$ where $x\\in \\mathbb{R}^m$ and $y\\in \\mathbb{R}^n$ are constant vectors.\n",
    "\n",
    "(This kind of derivative shows up frequently in machine learning, where $A$ is a \"weight\" matrix in a neural network.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We have $f(A) = x^TAy = \\sum_{p = 1}^m\\sum_{q = 1}^n x_p a_{pq} y_q$. It then follows that $\\frac{\\partial f}{\\partial a_{ij}} = x_iy_j$. We can then write \n",
    "$$\n",
    "\\nabla_A f = \\begin{pmatrix} x_1y_1 & x_1y_2 & \\cdots \\\\ x_2y_1 & x_2y_2 & \\cdots \\\\ \\vdots & \\vdots & \\ddots \\end{pmatrix}\n",
    "$$\n",
    "We can then identify this matrix as \n",
    "$$\n",
    "\\nabla_A f = \\begin{pmatrix} x_1 \\\\ \\vdots \\\\ x_m\\end{pmatrix} \\begin{pmatrix} y_1 & \\cdots & y_n \\end{pmatrix} = \\boxed{xy^T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (10 points)\n",
    "\n",
    "Suppose that we minimize the length of a vector along a line:\n",
    "$$\n",
    "\\min_{\\alpha \\in \\mathbb{R}} \\Vert u + \\alpha v \\Vert\n",
    "$$\n",
    "for some nonzero vectors $u, v \\in \\mathbb{R}^n$, finding the minimizer $\\hat{\\alpha}$.\n",
    "\n",
    "**(a)** If we write this in the form of a \"standard\" least-square problem $\\min_x \\Vert b - Ax \\Vert$, what are $A$, $b$, and $x$ in terms of the above?\n",
    "\n",
    "**(b)** Solve the normal equations to find an explicit solution $\\hat{\\alpha}$.\n",
    "\n",
    "**(c)** At this minimum, $u + \\hat{\\alpha} v$ is orthogonal to what vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** We can identify the minimization problem $\\min_{\\alpha \\in \\mathbb{R}} \\Vert u + \\alpha v \\Vert$ with the \"standard\" least-square problem by letting $b = u$, $A = -v$ (an $n\\times 1$ matrix), and $x=\\alpha$. \n",
    "\n",
    "**(b)** The normal equations are $A^TA\\hat{x}=A^T b$. In our notation, this becomes\n",
    "$$\n",
    "v^T v \\hat{\\alpha} = -v^T u \\implies \\hat{\\alpha} = -\\frac{v^T u}{v^T v}\n",
    "$$\n",
    "since $\\hat{\\alpha}$ is a scalar. \n",
    "\n",
    "**(c)** If you draw a picture, you will immediately see the point on the line that is closest to the origin to have a position vector which is orthogonal to the direction vector $v$.  In fact, this was precisely the thinking that led us from least-squares to orthogonal projection!  Explicitly, the error vector $e = b-Ax$ in a least-squares solution is always in the left nullspace of $A$, $N(A^T)$, because least-squares is equivalent to orthogonal projection. This gives us immediately that $\\boxed{v^T(u+\\hat{\\alpha} v) = 0}$.\n",
    "\n",
    "We can check this for our $\\hat{\\alpha}$ from part (b):\n",
    "$$\n",
    "v^T(u+\\hat{\\alpha}v) = v^Tu - \\frac{v^T u}{v^T v} v^Tv = v^Tu - v^Tu = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (15 points)\n",
    "\n",
    "Suppose that we have $m$ data points $\\{ (a_1, b_1), (a_2, b_2), \\ldots, (a_m, b_m) \\}$ that we want to perform a least-square fit to a function of the following form:\n",
    "\n",
    "$$\n",
    "f(a) = x_1 + x_2 a + x_3 a^2 + x_4 (a-1)^2\n",
    "$$\n",
    "\n",
    "That is, we want to minimize $\\sum_{i=1}^m [b_i - f(a_i)]^2$ over all possible $x \\in \\mathbb{R}^4$.\n",
    "\n",
    "**(a)** Formulate this in matrix form as in class: we are minimizing $\\Vert b - Ax \\Vert$ for what matrix $A$ and vector $b$?\n",
    "\n",
    "**(b)** Give the rank of $A$ and $A^T A$ and a basis for $N(A) = N(A^T A)$ (assuming that our data has at least 4 distinct $a_i$ values).  What does this tell you about the solutions to the normal equations $A^T A \\hat{x} = A^T b$ for the fit coefficients $\\hat{x}$?\n",
    "\n",
    "**(c)** Modify the following Julia code to create your matrix $A$ from the given data vectors $a$ and $b$ (see also the polynomial fitting examples in the lecture notes) and plot your least-square fit.\n",
    "\n",
    "**(d)** If the least-square solution is not unique, Julia's `x̂ = A \\ b` finds the `x̂` with **minimum length**, i.e. it minimizes $\\Vert \\hat{x} \\Vert$ over all possible solutions to $A^T A \\hat{x} = A^T b$.  In this problem, that means that Julia's `x̂` must be **orthogonal to what vector(s)**?  (Hint: see problem 3.) Check that this is true of the result that Julia gives you below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution \n",
    "\n",
    "**(a)** We can formulate $\\sum_{i=1}^m [b_i - f(a_i)]^2$  in matrix form $\\Vert b - Ax \\Vert$, by letting\n",
    "$$\n",
    "b = \\begin{pmatrix} b_1 \\\\ \\vdots \\\\ b_m \\end{pmatrix}, \\;\\;\n",
    "A = \\begin{pmatrix} 1 & a_1 & a_1^2 & (a_1-1)^2 \\\\ 1 & a_2 & a_2^2 & (a_2-1)^2 \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & a_m & a_m ^2 & (a_m-1)^2 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**(b)** Firstly we note that $(a_i-1)^2 = a_i^2 - 2a_i + 1$, and so the fourth column of $A$ is a linear combination of the first three columns. The first three columns are linearly independent, and so $rk(A) = 3$. We showed that $N(A) = N(A^TA)$ on the last pset, and since $A$ and $A^TA$ both have four columns, this means that $rk(A^TA) = 3$ also. It also allows us to conclude that $N(A) = N(A^TA)$ has dimension 1. Since we have found how to write the fourth column of $A$ as a linear combination of the first three columns, we can identify that a solution to $Ax_0 = 0$ is given by\n",
    "$$\n",
    "x_0 = \\begin{pmatrix} -1 \\\\ 2 \\\\ -1 \\\\ 1 \\end{pmatrix}.\n",
    "$$\n",
    "This vector then forms a basis for $N(A) = N(A^TA)$. \n",
    "\n",
    "We know that $A^Tb$ is always in the column space of $A^TA$, since we saw in class that $C(A^T A) = C(A^T)$. Since $A^TA$ is not full rank, this means that the least-squares solution always exists, but it is not unique since we will always be able to add a multiple of $x_0$ to any particular least squares solution and obtain another least squares solution.\n",
    "\n",
    "**(c)** We can use the code below to plot the least squares fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0.6, -0.1, 0.2, 0.3, 0.4, 0.35, 0.01, 0.5, 0.67, 0.88];\n",
    "b = [1.07943, 1.12779, 0.884219, 0.845884, 0.899928, 0.871585, 0.95691, 1.0084, 1.23807, 1.67931];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×4 Array{Float64,2}:\n",
       " 1.0   0.6   0.36    0.16  \n",
       " 1.0  -0.1   0.01    1.21  \n",
       " 1.0   0.2   0.04    0.64  \n",
       " 1.0   0.3   0.09    0.49  \n",
       " 1.0   0.4   0.16    0.36  \n",
       " 1.0   0.35  0.1225  0.4225\n",
       " 1.0   0.01  0.0001  0.9801\n",
       " 1.0   0.5   0.25    0.25  \n",
       " 1.0   0.67  0.4489  0.1089\n",
       " 1.0   0.88  0.7744  0.0144"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [ ones(a) a a.^2 (a.-1).^2 ] # concatenate columns, as in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Float64,1}:\n",
       " 0.246249\n",
       " 0.428064\n",
       " 1.35312 \n",
       " 0.743244"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x̂ = A \\ b # equivalent to solving normal equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "PyPlot.Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PyObject <matplotlib.legend.Legend object at 0x13a280240>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyPlot\n",
    "plot(a, b, \"ro\")\n",
    "â = linspace(-0.1, 1, 100)\n",
    "plot(â, x̂[1] .+ x̂[2] .* â .+ x̂[3] .* â.^2 +  x̂[4] .* (â .- 1).^2, \"b-\")\n",
    "xlabel(L\"a\")\n",
    "ylabel(L\"b\")\n",
    "legend([\"data\", \"fit\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d)** Julia will find the $\\hat{x}$ with minimum length. This means that Julia finds a solution to $A^TA\\hat{x}=A^Tb$ which is also gives the minimum value of $\\min_{\\alpha \\in \\mathbb{R}} \\Vert \\hat{x} + \\alpha x_0\\Vert$, where $x_0$ is the vector that spans our 1-dimensional $N(A^TA)=N(A)$. But we found in problem 3(c) that this $\\hat{x}$ must be orthogonal to $x_0$, which we can check explicitly for this example problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Array{Int64,1}:\n",
       " -1\n",
       "  2\n",
       " -1\n",
       "  1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x₀ = [-1, 2, -1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3306690738754696e-16"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x₀'x̂  # the dot product x₀ᵀ x̂"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is zero to within the computer's usual roundoff errors in the 15–16 digits.  So, as claimed, Julia is indeed finding the minimum-length least-square solution when $A$ is rank-deficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 (10 points)\n",
    "\n",
    "(From Strang 4.2, problem 10.)\n",
    "\n",
    "Project $a_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ onto the line spanned by $a_2 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.  Then project the result back onto the line spanned by $a_1$.  Multiply these projection matrices $P_1 P_2$: is this a projection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We can project $a_1$ onto the line spanned by $a_2$ by using the projection formula:\n",
    "$$\n",
    "P_2 a_1 = a_2 \\frac{a_2^T a_1}{a_2^T a_2} = \\frac{1}{5} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n",
    "$$\n",
    "We can then project the result back onto $a_1$ using the analogous projection formula:\n",
    "$$\n",
    "P_1 \\begin{pmatrix} \\frac{1}{5} \\\\ \\frac{2}{5} \\end{pmatrix} = a_1 \\frac{a_1^T \\begin{pmatrix} \\frac{1}{5} \\\\ \\frac{2}{5} \\end{pmatrix}}{a_1^T a_1} = \\frac{1}{5} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We can explicitly calculate the components of the projection matrix using the formulae:\n",
    "\\begin{align}\n",
    "P_2 &= \\frac{a_2 a_2^T}{a_2^Ta_2} = \\frac{1}{5}\\begin{pmatrix} 1 & 2 \\\\ 2 & 4 \\end{pmatrix}\\\\\n",
    "P_1 &= \\frac{a_1 a_1^T}{a_1^Ta_1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Multiplying these together yields:\n",
    "$$\n",
    "P = P_1P_2 = \\frac{1}{5}\\begin{pmatrix} 1 & 2 \\\\ 0 & 0 \\end{pmatrix}\n",
    "$$\n",
    "This is *not* a projection matrix. In particular, $P^2 = \\frac{1}{25} \\begin{pmatrix} 1 & 2 \\\\ 0 & 0 \\end{pmatrix} \\neq P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6 (10 points)\n",
    "\n",
    "(From Strang 4.2 problem 19.)\n",
    "\n",
    "To find the projection matrix onto the plane $x-y-2z=0$, choose two vectors in that plane (the null space of what matrix?) and make them columns of $A$ so that the plane is $C(A)$.  Then compute (by hand) the projection of the point $\\begin{pmatrix} 0 \\\\ 6 \\\\ 12 \\end{pmatrix}$ onto this plane, and check your result in Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "We first want to find two linearly independent vectors $(x,y,z)$ lying in the plane described by the equation $x-y-2z=0$. This means we want to find the nullspace of the $1 \\times 3$ matrix $A= (1 -1 -2)$. This matrix $A$ is in rref form, where the first column is the pivot column, and the last two columns are free columns. We can then seek two special solutions to $Ay = 0$ of the usual form:\n",
    "\\begin{align}\n",
    "y = \\begin{pmatrix} y_1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\;\\;\\; \\text{and}  \\;\\;\\; y = \\begin{pmatrix} y_2 \\\\ 0 \\\\ 1 \\end{pmatrix}\n",
    "\\end{align}\n",
    "We then solve to find $y_1 = 1$ and $y_2 =2$, so that a basis for the null space of $A$ is given by the two vectors\n",
    "\\begin{align}\n",
    "\\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\;\\;\\; \\text{and}  \\;\\;\\;  \\begin{pmatrix} 2 \\\\ 0 \\\\ 1 \\end{pmatrix}\n",
    "\\end{align}\n",
    "We can then define a $3\\times 2$ matrix $A$ as\n",
    "$$\n",
    "A = \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \n",
    "$$\n",
    "so that $C(A)$ is the plane in question. We can then compute the projection $p$ of $b = \\begin{pmatrix} 0 \\\\ 6 \\\\ 12 \\end{pmatrix}$ onto this plane by\n",
    "$$\n",
    "p = A\\underbrace{(A^T A)^{-1}A^Tb}_\\hat{x}\n",
    "$$\n",
    "As usual, we want to avoid explicitly inverting any matrices if we can, so we will find $\\hat{x}$ by solving the normal equations $A^TA\\hat{x} = A^Tb$.  We can then compute $p=A\\hat{x}$. We can compute\n",
    "\\begin{align}\n",
    "A^TA &= \\begin{pmatrix} 2 & 2 \\\\ 2 & 5 \\end{pmatrix},\\\\\n",
    "A^Tb &= \\begin{pmatrix} 6 \\\\ 12 \\end{pmatrix}.\\\\\n",
    "\\end{align}\n",
    "Then we have the equation \n",
    "\\begin{align}\n",
    "\\begin{pmatrix} 2 & 2 \\\\ 2 & 5 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 12 \\end{pmatrix}\n",
    "\\end{align}\n",
    "which gives us a linear system\n",
    "\\begin{align}\n",
    "2x_1 + 2x_2 &= 6\\\\\n",
    "2x_1 + 5x_2 &= 12\n",
    "\\end{align}\n",
    "with solution $x_1 =1 , x_2 =2 $. We can then find the projection $p=A\\hat{x}$:\n",
    "\\begin{align}\n",
    "p = \\begin{pmatrix} 1 & 2 \\\\ 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 1 \\\\ 2 \\end{pmatrix}\n",
    "\\end{align}\n",
    "We can then check this result explicitly in Julia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 5.0\n",
       " 1.0\n",
       " 2.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [2 0\n",
    "     0 2\n",
    "     1 -1]\n",
    "b = [0\n",
    "     6\n",
    "     12]\n",
    "p = A*(A\\b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we don't compute $(A^T A)^{-1}$ explicitly in Julia, either, because the solution to the normal equations is computed more accurately and efficiently by `A \\ x`.\n",
    "\n",
    "Of course, we could compute the explicit projection matrix if we wanted to, and we would get the same result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 5.0\n",
       " 1.0\n",
       " 2.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = A*inv(A'A)*A'\n",
    "P*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7 (10 points)\n",
    "\n",
    "(From Strang, section 4.2, problem 30.)\n",
    "\n",
    "**(a)** Find the projection matrix $P_C$ onto the column space $C(A)$ (after looking closely at the matrix!) for $A = \\begin{pmatrix} 3 & 6 & 6 \\\\ 4 & 8 & 8 \\end{pmatrix}$.\n",
    "\n",
    "**(b)** Find the 3 by 3 projection matrix $P_R$ onto the row space of $A$.  (You can google the formula for the inverse of a 2 by 2 matrix to try to shorten your algebra… though the fact that A is rank-deficient may give you some trouble… but there is an even simpler way to do it if you realize that the row space is `_____`-dimensional.)  Multiply $B = P_C A P_R$.  Your answer $B$ may be a little surprising at first — can you explain it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** Notice that $A$ only has one linearly independent column, so $C(A)$ is spanned by the vector $x_1 = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$. We can then find the projection matrix $P_C$ using the simple formula for projecting onto a vector:\n",
    "$$\n",
    "P_C = \\frac{x_1 x_1^T}{x_1^T x_1} = \\frac{1}{25}\\begin{pmatrix} 9 & 12 \\\\ 12 & 16 \\end{pmatrix} \n",
    "$$\n",
    "\n",
    "**(b)** Similarly, the row space is one dimensional and is spanned by the vector $x_2 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 2 \\end{pmatrix}$.  We can then find the projection matrix $P_R$ using the simple formula for projecting onto a vector:\n",
    "$$\n",
    "P_R = \\frac{x_2 x_2^T}{x_2^T x_2} = \\frac{1}{9}\\begin{pmatrix} 1 & 2 & 2 \\\\  2 & 4 & 4 \\\\ 2 & 4 & 4 \\end{pmatrix} \n",
    "$$\n",
    "We can then multiply $B = P_CAP_R$\n",
    "\\begin{align}\n",
    "B = P_CAP_R = \\frac{1}{25\\times 9}\\begin{pmatrix} 9 & 12 \\\\ 12 & 16 \\end{pmatrix} \\begin{pmatrix} 3 & 6 & 6 \\\\ 4 & 8 & 8 \\end{pmatrix} \\begin{pmatrix} 1 & 2 & 2 \\\\  2 & 4 & 4 \\\\ 2 & 4 & 4 \\end{pmatrix}\n",
    "= \\begin{pmatrix} 3 & 6 & 6 \\\\ 4 & 8 & 8 \\end{pmatrix} = A\n",
    "\\end{align}\n",
    "\n",
    "Since $A$ is rank one, we can write $A = uv^T$, where $u = x_1$ and $v = x_2$. Then\n",
    "$$\n",
    "AP_R = \\frac{(x_1x_2^T)(x_2x_2^T)}{(x_2^Tx_2)} = x_1x_2^T =A\\\\\n",
    "AP_R = \\frac{(x_1x_1^T)(x_1x_2^T)}{(x_1^Tx_1)} = x_1x_2^T =A\\\\\n",
    "$$\n",
    "\n",
    "So even better than $P_CAP_R = A$, we have *both* that $P_CA = A$ and $AP_R = A$ (and then $P_CAP_R = A$ follows from these two).\n",
    "\n",
    "  - Why is $P_CA = A$?  Well, two matrices $M, N$ of the same size are equal if and only if $Mx = Nx$ for all possible column vectors $x$ so that the multiplication makes sense.  So to see that $P_CA = A$, we only need to see that $P_CAx = Ax$ for any vector $x$; but $Ax$ is in the column space $C(A)$, so it's projection $P_CAx$ is itself, i.e. $P_CAx = Ax$.  So $P_CA = A$.\n",
    "  \n",
    "  - Similarly, to check that $AP_R = A$ we can just check that $AP_Rx = P_Rx$ for all possible $x$.  Any $x$ in $\\mathbb{R}^n$ (let's say that $A$ is $m \\times n$ can be written in the form $x = x_n + x_r$ with $x_n$ in the nullspace and $x_r$ in the row space of $A$ (because these spaces are orthogonal complements).  So it's enough to check that $AP_Rx_r = Ax_r$ and $AP_rx_n = Ax_n$.  In the first case, we have $P_Rx_r = x_r$ by definition of projection, so indeed $AP_Rx_r = Ax_r$.  In the second case, we have $P_Rx_n = 0$ because $N(A)$ and $C(A^T)$ are orthogonal; as $Ax_n = 0$ as well by the definition of nullspace, we have $AP_Rx_n = Ax_n (= 0)$.  So $AP_Rx = Ax$ for all $x$, so $AP_R = A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8 (10 points)\n",
    "\n",
    "Given two $m \\times n$ matrices $A$ and $B$ and two right-hand sides $b, c \\in \\mathbb{R}^m$, suppose that we want to minimize:\n",
    "$$\n",
    "f(x) = \\Vert b - Ax \\Vert^2 + \\Vert c - Bx \\Vert^2\n",
    "$$\n",
    "over $x \\in \\mathbb{R}^n$.  That is, we are minimizing the *sum* of two least-square fitting problems.\n",
    "\n",
    "**(a)** $\\Vert b \\Vert^2 + \\Vert c\\Vert^2 = \\Vert w \\Vert^2 $ for a vector $w\\in\\mathbb{R}^{2m}$.  Give such a $w$.\n",
    "\n",
    "**(b)** Write down a matrix equation $C \\hat{x} = d$ whose solution $\\hat{x}$ gives the minimum of $f(x)$.  Give explicit formulas for $C$ and $d$ in terms of $A, B, b, c$.  Hint: use your answer from (a) to convert this into a \"normal\" least-squares problem first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "**(a)** The vector $w = \\begin{pmatrix} b \\\\ c \\end{pmatrix}$ has $\\Vert b \\Vert^2 + \\Vert c\\Vert^2 = \\Vert w \\Vert^2$\n",
    "\n",
    "**(b)** We can then write $f(x) = \\Vert b - Ax \\Vert^2 + \\Vert c - Bx \\Vert^2 = \\Vert \\begin{pmatrix} b \\\\ c \\end{pmatrix} - \\begin{pmatrix} A \\\\ B \\end{pmatrix} x \\Vert^2$, so that the minimum of $f(x)$ is found at $\\hat{x}$ obeying $C\\hat{x} = d$, where:\n",
    "\\begin{align}\n",
    "C &= \\begin{pmatrix} A \\\\ B \\end{pmatrix}^T \\begin{pmatrix} A \\\\ B \\end{pmatrix} = \\begin{pmatrix} A^T & B^T \\end{pmatrix}\\begin{pmatrix} A \\\\ B \\end{pmatrix} = A^TA+ B^TB\\\\\n",
    "d &= \\begin{pmatrix} A \\\\ B \\end{pmatrix}^T \\begin{pmatrix} b \\\\ c \\end{pmatrix} = A^Tb + B^Tc\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
