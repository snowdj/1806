{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18.06 pset 10 - SOLUTIONS\n",
    "\n",
    "Due Wed. Nov. 14 at 10:55am."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (5+5)\n",
    "\n",
    "**(a)** Suppose that we compute the QR factorization $A = QR$ of an $m\\times m$ matrix $A$.  Show that the matrix $B = RQ$ has the same eigenvalues as $A$ ($B$ is ........ to $A$).\n",
    "\n",
    "**(b)** Start with a random $5\\times 5$ symmetric matrix $A = A^T$ and repeatedly perform the $QR \\rightarrow RQ$ transformation using the code below.  What does it look like the result is converging to?  Any relationship to the eigenvalues of $A$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "**(a)** Suppose that $A$ has a QR factorization $A=QR$. Since $A$ is a square, $m\\times m$ matrix, this means that it must have $m$ linearly independent columns, and $Q$ is an invertible matrix with $Q^{-1} = Q^T$. We can then write $R = Q^{-1}A$. Substituting this into the expression for $B$ yields:\n",
    "\\begin{align}\n",
    "\\boxed{B = RQ = Q^{-1}AQ},\n",
    "\\end{align}\n",
    "and so $B$ is similar to $A$. Hence $A$ and $B$ will have the same eigenvalues.\n",
    "\n",
    "**(b)** Suppose we have a symmetric matrix $A=A^T$. This will necessarily have real eigenvalues. We now use Julia to repeatedly perform the $QR \\rightarrow RQ$ transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " -6.22672\n",
       " -4.63669\n",
       " -1.61703\n",
       "  1.37237\n",
       "  2.8885 "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = randn(5,5)\n",
    "A = A + A' # make it symmetric\n",
    "eigvals(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " -6.22639     -0.0227726   -0.00136184   3.70783e-6  -1.49538e-6\n",
       " -0.0227726   -4.63686     -0.0344993    8.26629e-5   5.30815e-5\n",
       " -0.00136184  -0.0344993    2.88784     -0.0465889   -0.00757203\n",
       "  3.70783e-6   8.26629e-5  -0.0465889   -1.50412     -0.568641  \n",
       " -1.49538e-6   5.30815e-5  -0.00757203  -0.568641     1.25996   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " -6.22672      -0.00119428   -6.28478e-7    4.6894e-12   -4.11193e-13\n",
       " -0.00119428   -4.63669      -0.000303497   2.42182e-9    2.79075e-10\n",
       " -6.28478e-7   -0.000303497   2.8885       -0.000142557  -4.52118e-6 \n",
       "  4.6898e-12    2.42182e-9   -0.000142557  -1.61265      -0.1144     \n",
       " -4.12029e-13   2.79075e-10  -4.52118e-6   -0.1144        1.36799    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " -6.22672      -6.26032e-5   -2.90014e-10  -3.68424e-16   8.47456e-16\n",
       " -6.26032e-5   -4.63669      -2.67171e-6    6.48235e-14   1.17727e-15\n",
       " -2.90013e-10  -2.67171e-6    2.8885       -4.31172e-7   -2.65191e-9 \n",
       "  6.51665e-18   6.46804e-14  -4.31172e-7   -1.61686      -0.0222117  \n",
       " -1.11522e-19   1.44101e-15  -2.65191e-9   -0.0222117     1.37221    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " -6.22672      -3.28161e-6   -1.34986e-13  -3.69852e-16   8.49788e-16\n",
       " -3.28161e-6   -4.63669      -2.35194e-8    1.43278e-16  -2.64615e-16\n",
       " -1.33827e-13  -2.35194e-8    2.8885       -1.30352e-9   -1.55397e-12\n",
       "  9.08962e-24   1.72158e-18  -1.30352e-9   -1.61702      -0.00430672 \n",
       " -3.01649e-26   7.43558e-21  -1.55443e-12  -0.00430672    1.37237    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " -6.22672      -1.72019e-7   -1.2204e-15   -3.68864e-16   8.50217e-16\n",
       " -1.72019e-7   -4.63669      -2.07044e-10   1.4125e-16   -2.64788e-16\n",
       " -6.17548e-17  -2.07044e-10   2.8885       -3.93938e-12  -4.48272e-16\n",
       "  1.26803e-29   4.58167e-23  -3.94073e-12  -1.61703      -0.000835006\n",
       " -8.15887e-33   3.83666e-26  -9.11116e-16  -0.000835006   1.37237    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " -6.22672      -9.01711e-9   -1.15867e-15  -3.68673e-16   8.503e-16  \n",
       " -9.01711e-9   -4.63669      -1.82299e-12   1.4119e-16   -2.6482e-16 \n",
       " -2.84969e-20  -1.82263e-12   2.8885       -1.05614e-14   4.62006e-16\n",
       "  1.76895e-35   1.21933e-27  -1.19134e-14  -1.61703      -0.000161894\n",
       " -2.20678e-39   1.97966e-31  -5.34042e-19  -0.000161894   1.37237    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " -6.22672      -4.72671e-10  -1.15864e-15  -3.68636e-16   8.50316e-16\n",
       " -4.72669e-10  -4.63669      -1.63999e-14   1.41179e-16  -2.64826e-16\n",
       " -1.31499e-23  -1.60449e-14   2.8885        1.31597e-15   4.6248e-16 \n",
       "  2.46775e-41   3.24501e-32  -3.6016e-17   -1.61703      -3.13887e-5 \n",
       " -5.96879e-46   1.02148e-36  -3.13023e-22  -3.13887e-5    1.37237    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " -6.22672      -2.47786e-11  -1.15864e-15  -3.68629e-16   8.50319e-16\n",
       " -2.47769e-11  -4.63669      -4.96274e-16   1.41176e-16  -2.64827e-16\n",
       " -6.06807e-27  -1.41245e-16   2.8885        1.35188e-15   4.62469e-16\n",
       "  3.4426e-47    8.63598e-37  -1.08882e-19  -1.61703      -6.08577e-6 \n",
       " -1.61441e-52   5.27068e-42  -1.83475e-25  -6.08577e-6    1.37237    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " -6.22672      -1.30045e-12  -1.15864e-15  -3.68627e-16   8.5032e-16 \n",
       " -1.29879e-12  -4.63669      -3.56272e-16   1.41176e-16  -2.64828e-16\n",
       " -2.80012e-30  -1.24339e-18   2.8885        1.35199e-15   4.62467e-16\n",
       "  4.80256e-53   2.2983e-41   -3.29165e-22  -1.61703      -1.17993e-6 \n",
       " -4.36659e-59   2.7196e-47   -1.07542e-28  -1.17993e-6    1.37237    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5×5 Array{Float64,2}:\n",
       " -6.22672      -6.97474e-14  -1.15864e-15  -3.68627e-16   8.5032e-16 \n",
       " -6.80813e-14  -4.63669      -3.5504e-16    1.41176e-16  -2.64828e-16\n",
       " -1.29212e-33  -1.09458e-20   2.8885        1.35199e-15   4.62467e-16\n",
       "  6.69975e-59   6.1165e-46   -9.95114e-25  -1.61703      -2.2877e-7  \n",
       " -1.18105e-65   1.40327e-52  -6.30348e-32  -2.2877e-7     1.37237    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "B = A\n",
    "for i = 1:100\n",
    "    Q, R = qr(B)\n",
    "    B = R * Q\n",
    "    if i % 10 == 0\n",
    "        display(B) # print B every 10 iterations\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each step in this iteration, we know from part **(a)** that $B$ and $A$ will be similar matrices, and so they will have the same eigenvalues. However, we find that the result converges to a diagonal matrix. The entries on the diagonal are in fact the eigenvalues of the original matrix $A$, arranged in descending order by magnitude!\n",
    "\n",
    "(The fact that the eigenvalues on the diagonal are sorted by magnitude is a hint that this iteration is somehow related to the power method.   This iteration is a simplified version of the [QR algorithm](https://en.wikipedia.org/wiki/QR_algorithm) that sophisticated library functions like `eigvals` use to compute eigenvalues, and it turns out that it is indeed essentially a power method \"in disguise.\"  Proving this is outside the scope of 18.06, however.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (10+10)\n",
    "\n",
    "Suppose that $A$ is a diagonalizable matrix with eigenvalues $|\\lambda_1| > |\\lambda_2| > \\cdots$ and real eigenvectors $x_1, x_2, \\ldots$.\n",
    "\n",
    "If we apply the power method starting with a random $x$ (repeatedly replacing $x$ by $Ax / \\Vert Ax \\Vert$), we saw in class that this would converge to an eigenvector $x_1$ (with $\\Vert x_1 \\Vert = 1$) corresponding to $\\lambda_1$ (the largest-magnitude eigenvector).\n",
    "\n",
    "Suppose that, after finding $x_1$ (and $\\lambda_1$) in this way, we now perform a modified power method: start with a random $y$, and repeatedly replace $y$ with $(I - x_1 x_1^T) Ay / \\Vert (I - x_1 x_1^T) Ay \\Vert$.  That is, we multiply by $A$, **project orthogonal** to $x_1$, and then normalize. \n",
    "\n",
    "**(a)** Write an equation for what $y$ converges to in terms of the eigenvectors $x_1, x_2, \\ldots$ of $A$.\n",
    "\n",
    "(Hint: imagine the initial $y$ is written in the basis of the eigenvectors, and work out what each iteration does in this basis.)\n",
    "\n",
    "**(b)** If $Q = \\begin{pmatrix} x_1 & y \\end{pmatrix}$ (which has 2 orthonormal columns), then express the $2\\times 2$ matrix $B = Q^T A Q$ have in terms of the eigenvalues and eigenvectors of $A$.  In particular, once you compute $B$, what do you learn about the eigenvalues of $A$?\n",
    "\n",
    "Check your answer in Julia for a  $5 \\times 5$ matrix with eigenvalues $\\lambda = 5,4,3,2,1$ and randomly chosen eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "**(a)** Firstly, let's write $y$ in the basis of eigenvectors (where $\\Vert x_1\\Vert = 1$): \n",
    "\\begin{align}\n",
    "y = c_1x_1 + c_2x_2 + ... + c_mx_m\n",
    "\\end{align}\n",
    "multiplying by $A$ yields:\n",
    "\\begin{align}\n",
    "Ay = c_1\\lambda_1x_1 + c_2\\lambda_2x_2 + ... + c_m\\lambda_m x_m\n",
    "\\end{align}\n",
    "and then projecting orthogonal to $x_1$ yields:\n",
    "\\begin{align}\n",
    "(I-x_1x_1^T)Ay &= c_1\\lambda_1(I-x_1x_1^T)x_1 + c_2\\lambda_2(I-x_1x_1^T)x_2 + ... + c_m\\lambda_m (I-x_1x_1^T)x_m\\\\\n",
    "&= c_2\\lambda_2(I-x_1x_1^T)x_2 + ... + c_m\\lambda_m (I-x_1x_1^T)x_m,\n",
    "\\end{align}\n",
    "since $(I-x_1x_1^T)x_1 = x_1-x_1 = 0$. We can then apply $(I-x_1x_1^T)A$ again to yield\n",
    "\\begin{align}\n",
    "\\left[(I-x_1x_1^T)A\\right]^2y &= c_2\\lambda_2(I-x_1x_1^T)(A-Ax_1x_1^T)x_2 + ... + c_m\\lambda_m(I-x_1x_1^T)(A-Ax_1x_1^T)x_m\\\\\n",
    "&= c_2\\lambda_2^2(I-x_1x_1^T)x_2 + ... +  c_m\\lambda_m^2(I-x_1x_1^T)x_m,\n",
    "\\end{align}\n",
    "where the second line follows from the fact that, for example:\n",
    "\\begin{align}\n",
    "(I-x_1x_1^T)(A-Ax_1x_1^T)x_k &= (I-x_1x_1^T)(\\lambda_kx_k - \\lambda_1(x_1^Tx_k) x_1)\\\\\n",
    "&= \\lambda_k(I-x_1x_1^T)x_k - \\lambda_1(x_1^Tx_k) (I-x_1x_1^T) x_1\\\\\n",
    "&= \\lambda_k(I-x_1x_1^T)x_k.\n",
    "\\end{align}\n",
    "\n",
    "We can then deduce that \n",
    "\\begin{align}\n",
    "\\left[(I-x_1x_1^T)A\\right]^n y = c_2\\lambda_2^n(I-x_1x_1^T)x_2 + ... +  c_m\\lambda_m^n(I-x_1x_1^T)x_m = \\lambda_2^n\\left[c_2(I-x_1x_1^T)x_2 + ... +  c_m\\left(\\frac{\\lambda_m}{\\lambda_2}\\right)^n(I-x_1x_1^T)x_m\\right]\n",
    "\\end{align}\n",
    "\n",
    "So as $n\\to \\infty$, $\\left[(I-x_1x_1^T)A\\right]^n y$ becomes dominated by a term proportional to $(I-x_1x_1^T)x_2$. By normalizing, we cancel out the exponentially growing factor of $\\lambda_2^n$, and so our original vector $y$ will converge onto the normalized projection of the eigenvector $x_2$ orthogonal to the eigenvector $x_1$, i.e.\n",
    "\\begin{align}\n",
    "\\boxed{ y \\to \\frac{(I-x_1x_1^T)x_2}{\\Vert (I-x_1x_1^T)x_2 \\Vert}}\n",
    "\\end{align}\n",
    "\n",
    "**(b)** Let $Q = \\begin{pmatrix} x_1 & y \\end{pmatrix}$. We can then define a $2\\times 2$ matrix $B = Q^T A Q$. Then:\n",
    "\\begin{align}\n",
    "B &= \\begin{pmatrix} x_1^T \\\\ y^T \\end{pmatrix} A  \\begin{pmatrix} x_1 & y \\end{pmatrix} \\\\ \n",
    "&= \\begin{pmatrix} x_1^T \\\\ y^T \\end{pmatrix} \\begin{pmatrix} \\lambda_1 x_1 & A y \\end{pmatrix}\\\\\n",
    "&= \\begin{pmatrix} \\lambda_1 x_1^Tx_1 & x_1^T A y \\\\ 0 & y^TAy \\end{pmatrix}\n",
    "\\end{align}\n",
    "So $B$ is an upper triangular matrix. The upper left entry is $\\lambda_1 x_1^Tx_1 = \\lambda_1$ (the largest eigenvalue), while the lower right entry is $y^T Ay$. We firstly find $Ay$:\n",
    "\\begin{align}\n",
    "Ay = \\frac{A(I-x_1x_1^T)x_2}{\\Vert (I-x_1x_1^T)x_2 \\Vert} &= \\frac{\\lambda_2x_2-\\lambda_1x_1(x_1^Tx_2)}{\\Vert (I-x_1x_1^T)x_2 \\Vert} \\\\\n",
    "&= \\frac{\\lambda_2(x_2-x_1(x_1^Tx_2)) + (\\lambda_2 -\\lambda_1)x_1(x_1^Tx_2)}{\\Vert (I-x_1x_1^T)x_2 \\Vert}\\\\ \n",
    "&= \\lambda_2y + \\frac{(\\lambda_2-\\lambda_1)(x_1^Tx_2)x_1}{\\Vert (I-x_1x_1^T)x_2 \\Vert}.\n",
    "\\end{align}\n",
    "Then since $y$ and $x_1$ are orthogonal to each other, we can see that $y^TAy = \\lambda_2$. \n",
    "\n",
    "Hence $B$ is an upper triangular matrix, with the eigenvalues $\\lambda_1$ and $\\lambda_2$ on the diagonal. \n",
    "\n",
    "We can check this answer in Julia for a  $5 \\times 5$ matrix with eigenvalues $\\lambda = 5,4,3,2,1$ and randomly chosen eigenvectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 5.0          -0.650165\n",
       " 4.31792e-16   4.0     "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = mapslices(normalize, randn(5,5), 1) # random eigenvectors (normalized)\n",
    "A = X * Diagonal([5,4,3,2,1]) / X\n",
    "\n",
    "# first power iteration\n",
    "x₁ = randn(5)\n",
    "for i = 1:1000 # plenty of iterations\n",
    "    x₁ = normalize(A*x₁)\n",
    "end\n",
    "\n",
    "# modified power iteration\n",
    "y = randn(5)\n",
    "for i = 1:1000 # plenty of iterations\n",
    "    y = A*y\n",
    "    y = normalize(y - x₁*(x₁'y)) # normalized ⟂ projection\n",
    "end\n",
    "\n",
    "Q = [x₁ y]\n",
    "B = Q'*A*Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $B$ is indeed upper triangular, with the two largest eigenvalues (5 and 4) on its diagonal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (5+5+10+10+5 points)\n",
    "\n",
    "Suppose that we have a second-order system of ODEs, of the form:\n",
    "$$\n",
    "\\ddot{x} = \\frac{d^2 x}{dt^2} = A x\n",
    "$$\n",
    "\n",
    "where $A$ is an $m \\times m$ matrix and we use $\\dot{x}$ to denote $dx/dt$ and $\\ddot{x}$ to denote $d^2 x/dt^2$.\n",
    "\n",
    "If $A=a$ and $x$ are scalars, this becomes the scalar ODE $\\frac{d^2 x}{dt^2} = a x$.  If we let $\\kappa = \\sqrt{a}$, then it is easy to check that the solutions (assuming $a \\ne 0$) are of the form $x(t) = c e^{+\\kappa t} + d e^{-\\kappa t}$ where $c$ and $d$ are determined by initial conditions $x(0)$ and $\\dot{x}(0)$.\n",
    "\n",
    "**(a)** For the scalar case, show that the same solution can also be written $x(t) = \\alpha \\cosh(\\kappa t) + \\beta \\sinh(\\kappa t)$ in terms of the [hyperbolic functions](https://en.wikipedia.org/wiki/Hyperbolic_function) cosh and sinh.  What are $\\alpha$ and $\\beta$ in terms of $c$ and $d$ from the solution above?  What are $\\alpha$ and $\\beta$ in terms of $x(0)$ and $\\dot{x}(0)$?\n",
    "\n",
    "**(b)** For the scalar case, if $a < 0$ then we have a purely imaginary $\\sqrt{a} = \\kappa = i\\omega$ for a real $\\omega$.   Using your answers from the previous part, write $x(t)$ as a purely real function (for real initial conditions) in terms of $\\cos(\\omega t)$ and $\\sin(\\omega t)$.   Note that $\\cosh(iy) = \\cos(y)$ and $\\sinh(iy) = i\\sin(y)$.\n",
    "\n",
    "**(c)** If $v$ is an eigenvector of $A$ with eigenvalue $\\lambda$, find a solution $x(t)$ of $\\ddot{x} = A x$ given by $v$ multiplied by some scalar function $f(t)$.   (Hint: for $v$, remember that $A$ acts like a scalar.  Your function can be written in terms of $\\cosh$ and $\\sinh$ of ...?)\n",
    "\n",
    "**(d)** Suppose $A$ is diagonalizable, with a basis eigenvectors $v_1, \\ldots, v_m$ and eigenvalues $\\lambda_1, \\ldots, \\lambda_m$.   Expand your solution $x(t)$ in the basis of these eigenvectors by adding up your answers from (c).   If we write the initial conditions in the basis of the eigenvectors, $x(0) = \\sum_k c_k v_k$ and $\\dot{x}(0) = \\sum_k d_k v_k$ for coefficients c and d, you should be able to give an **explicit formula for x(t)** in terms of the $c_k$, $d_k$, $v_k$, and $\\lambda_k$.\n",
    "\n",
    "**(e)** You will get *sinusoidally oscillating* (not growing or decaying) solutions if all of the eigenvalues of A are .........?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "**(a)** In the scalar case, $\\frac{d^2 x}{dt^2} = a x$, we have a solution $x(t) = c e^{+\\kappa t} + d e^{-\\kappa t}$. We can equivalently write this as $x(t) = \\alpha \\cosh(\\kappa t) + \\beta \\sinh(\\kappa t)$. To see this, we substitute the definition of $\\sinh$ and $\\cosh$ into this expression:\n",
    "\\begin{align*}\n",
    "\\alpha \\cosh(\\kappa t) + \\beta \\sinh(\\kappa t) \n",
    "&= \\alpha\\left(\\frac{e^{\\kappa t}+e^{-\\kappa t}}{2}\\right) + \\beta \\left(\\frac{e^{\\kappa t}-e^{-\\kappa t}}{2}\\right)\\\\\n",
    "&= \\left(\\frac{\\alpha + \\beta}{2}\\right)e^{\\kappa t} + \\left(\\frac{\\alpha - \\beta}{2}\\right)e^{-\\kappa t}.\n",
    "\\end{align*}\n",
    "So the two forms of the solution are equivalent, with $c = \\frac{\\alpha + \\beta}{2}$ and $d = \\frac{\\alpha - \\beta}{2}$. We can then express $\\alpha$ and $\\beta$ in terms of $c$ and $d$:\n",
    "\\begin{align}\n",
    "\\boxed{\\alpha = c+d\\\\\n",
    "\\beta = c-d}\n",
    "\\end{align}\n",
    "\n",
    "Using our solution in the form $x(t) = \\alpha \\cosh(\\kappa t) + \\beta \\sinh(\\kappa t)$, we can then find $\\alpha$ and $\\beta$ in terms of the initial conditions:\n",
    "\\begin{align}\n",
    "x(0) &= \\alpha \\cosh{(0)} + \\beta\\sinh{(0)} = \\alpha\\\\\n",
    "\\dot{x}(0) &= \\kappa\\alpha \\sinh{(0)} + \\kappa\\beta\\cosh{(0)} = \\kappa\\beta\n",
    "\\end{align}\n",
    "and so\n",
    "\\begin{align}\n",
    "\\boxed{\\alpha = x(0), \\;\\; \\beta = \\frac{1}{\\kappa}\\dot{x}(0) }\n",
    "\\end{align}\n",
    "\n",
    "**(b)** If $a<0$, then $\\kappa = i\\omega$ is pure imaginary. We then have:\n",
    "\\begin{align}\n",
    "x(t) &= x(0) \\cosh{i\\omega t} + \\frac{1}{i\\omega}\\dot{x}(0) \\sinh{i\\omega t}\\\\\n",
    "&= x(0) \\cos{\\omega t} + \\frac{i}{i\\omega}\\dot{x}(0)\\sin{\\omega t}\\\\\n",
    "&= x(0) \\cos{\\omega t} + \\frac{1}{\\omega}\\dot{x}(0)\\sin{\\omega t}\n",
    "\\end{align}\n",
    "\n",
    "**(c)** If $v$ is an eigenvector of $A$ with eigenvalue $\\lambda$, then consider a solution of the form $x(t) = f(t)v$, where $f(t)$ is a scalar function of time. Substituting this into the differential equation yields:\n",
    "\\begin{align}\n",
    "\\ddot{x}=Ax &\\implies \\ddot{f}(t)v = f(t)Av\\\\\n",
    "&\\implies \\ddot{f}(t)v= \\lambda f(t) v\\\\\n",
    "&\\implies (\\ddot{f}(t) - \\lambda f(t))v = 0\\\\\n",
    "&\\implies \\boxed{\\ddot{f}(t) = \\lambda f(t)}\n",
    "\\end{align}\n",
    "and so\n",
    "\\begin{align}\n",
    "\\boxed{f(t) = \\alpha \\cosh(\\sqrt{\\lambda} t) + \\beta \\sinh(\\sqrt{\\lambda} t)}\n",
    "\\end{align}\n",
    "\n",
    "**(d)** Suppose $A$ is diagonalizable, with a basis of eigenvectors $v_1, \\ldots, v_m$ and eigenvalues $\\lambda_1, \\ldots, \\lambda_m$. We then have a family of solutions to the differential equation of the form \n",
    "\\begin{align}\n",
    "x_k(t) = \\left(\\alpha_k \\cosh(\\sqrt{\\lambda_k} t) + \\beta_k \\sinh(\\sqrt{\\lambda_k} t)\\right) v_k, \\;\\; \\text{for} \\;\\; k = 1,...,m.\n",
    "\\end{align}\n",
    "\n",
    "Since the differential equation is linear, we can then sum over $k$ to obtain a general solution of the form:\n",
    "\\begin{align}\n",
    "\\boxed{x(t) = \\sum_{k=1}^m \\left(\\alpha_k \\cosh(\\sqrt{\\lambda_k} t) + \\beta_k \\sinh(\\sqrt{\\lambda_k} t)\\right) v_k}\n",
    "\\end{align}\n",
    "We can then apply the initial conditions $x(0) = \\sum_k c_k v_k$ and $\\dot{x}(0) = \\sum_k d_k v_k$:\n",
    "\\begin{align}\n",
    "x(0) &= \\sum_{k=1}^m \\left(\\alpha_k \\cosh(0) + \\beta_k \\sinh(0)\\right) v_k = \\sum_k c_k v_k \\\\\n",
    "\\dot{x}(0) &= \\sum_{k=1}^m \\left(\\alpha_k\\sqrt{\\lambda_k} \\sinh(0) + \\beta_k\\sqrt{\\lambda_k} \\cosh(0)\\right) v_k = \\sum_k d_k v_k. \n",
    "\\end{align}\n",
    "We can then identify that $\\alpha_k = c_k$ and $\\beta_k = \\frac{d_k}{\\sqrt{\\lambda_k}}$ to give and explicit formula for $x(t)$:\n",
    "\\begin{align}\n",
    "\\boxed{x(t) = \\sum_{k=1}^m \\left(c_k \\cosh(\\sqrt{\\lambda_k} t) + \\frac{d_k}{\\sqrt{\\lambda_k}} \\sinh(\\sqrt{\\lambda_k} t)\\right) v_k}\n",
    "\\end{align}\n",
    "\n",
    "**(e)** You will get *sinusoidally oscillating* solutions provided that $\\sqrt{\\lambda_k}$ is pure imaginary for all $k$, i.e. if $\\;\\boxed{\\lambda_k \\mbox{ is real and }\\lambda_k <0 \\mbox{ for all } k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 (8+8)\n",
    "\n",
    "In chemistry, the [stoichiometry matrix](https://en.wikipedia.org/wiki/Stoichiometry#Stoichiometry_matrix) is often used to describe a set of $m$ reactions among $n$ different chemical \"species\" (e.g. H₂O, C₈H₁₀N₄O₂, and so on).\n",
    "\n",
    "For example, consider the following 3 (fictitious) chemical reactions among 4 species, labeled $x_1, x_2, x_3, x_4$:\n",
    "\n",
    "$$\n",
    "x_1 + 2x_2 \\longleftrightarrow 3 x_2 + 2x_4 \\\\\n",
    "2x_2 + 4x_3 \\longleftrightarrow x_1 + x_4 \\\\\n",
    "x_1 + 4x_3 \\longleftrightarrow 5x_4\n",
    "$$\n",
    "\n",
    "which would be represented by the stoichiometry matrix\n",
    "\n",
    "$$\n",
    "S = \\begin{pmatrix} -1 & 3-2 & 0 & 2 \\\\\n",
    "1 & -2 & -4 & 1 \\\\\n",
    "-1 & 0 & -4 & 5 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "whose rows are the reactions and whose columns are the species.  (Some authors use the transpose of this matrix instead.)\n",
    "\n",
    "If we use a vector $\\vec{x} \\in \\mathbb{R}^4$ to represent the *concentrations* of each of these four species, and a vector $\\vec{r} \\in \\mathbb{R}^3$ to represent the *rates* of each reaction, then the rate of change of the concentrations is given by the system of ordinary differential equations (ODEs):\n",
    "\n",
    "$$\n",
    "\\frac{d\\vec{x}}{dt} = S^T \\vec{r}\n",
    "$$\n",
    "\n",
    "(where the rates $\\vec{r}$ are *not* generally constant: they may depend on the concentrations $\\vec{x}$ in a complicated way … so you *can't* solve this just by multiplying the right-hand side by $t$).\n",
    "\n",
    "**(a)** Describe (find a basis for) *all* possible reaction rates $\\vec{r}$ for which $\\frac{d\\vec{x}}{dt} = 0$ (the system is in *steady state*).\n",
    "\n",
    "**(b)** Certain linear combinations of the concentrations are *conserved*: there are some (time-independent) vectors $\\vec{v} \\in \\mathbb{R}^4$ for which $\\frac{d(\\vec{v}^T \\vec{x})}{dt} = 0$ for *all* possible rate vectors $\\vec{r}$.   If $\\vec{v}$ doesn't depend on $t$, then $\\frac{d(\\vec{v}^T \\vec{x})}{dt}$ is `______` times $\\frac{d\\vec{x}}{dt}$. These vectors $\\vec{v}$ all lie within the `___`-dimensional `___________` space of $S$.  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "**(a)** The system is in steady state if $\\frac{d\\vec{x}}{dt} = S^T \\vec{r} = 0$. This occurs if and only if $\\vec{r}$ is in the **left nullspace** of $S$: $\\boxed{\\vec{r}\\in N(S^T)}$. To find a basis fo4 $N(S^T)$, we can firstly put $S^T$ into rref form:\n",
    "\\begin{align}\n",
    "S^T = \\begin{pmatrix} -1 & 1 & -1\\\\ 1 & -2 & 0 \\\\0 & -4 & -4 \\\\ 2 & 1& 5 \\end{pmatrix}\n",
    "\\rightarrow \\begin{pmatrix} -1 & 1 & -1\\\\ 0 & -1 & -1 \\\\0 & -4 & -4 \\\\ 0 & 3 & 3 \\end{pmatrix}\n",
    "\\rightarrow \\begin{pmatrix} 1 & -1 & 1\\\\ 0 & 1 & 1 \\\\0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n",
    "\\rightarrow \\begin{pmatrix} 1 & 0 & 2\\\\ 0 & 1 & 1 \\\\0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "We see that $S^T$ has two pivot columns, and one free column. We can deduce that $\\mathrm{rank}(S)=\\mathrm{rank}(S^T)=2$. Since $S^T$ has three columns, it will have a one dimensional nullspace. We can then find the special solution of $S^Tx = 0$, which is $x= \\begin{pmatrix} -2 \\\\ -1 \\\\ 1 \\end{pmatrix}$. \n",
    "\n",
    "Therefore the reaction rates for which $\\vec{r}$ for which $\\frac{d\\vec{x}}{dt} = 0$ are of the form\n",
    "\\begin{align}\n",
    "\\boxed{\\vec{r} = r_0\\begin{pmatrix} -2 \\\\ -1 \\\\ 1 \\end{pmatrix}}\n",
    "\\end{align}\n",
    "for all scalars $r_0\\in\\mathbb{R}$.\n",
    "\n",
    "**(b)** We want to find $\\vec{v}$ independent of time, for which $\\frac{d(\\vec{v}^T \\vec{x})}{dt} = 0$ for *all* possible rate vectors $\\vec{r}$. If $\\vec{v}$ doesn't depend on $t$, then \n",
    "\\begin{align}\n",
    "\\frac{d(\\vec{v}^T \\vec{x})}{dt} = \\vec{v}^T\\frac{d\\vec{x}}{dt} = \\vec{v}^T S^T \\vec{r} = 0.\n",
    "\\end{align}\n",
    "We seek $\\vec{v}$ for which this is true for *all* $\\vec{r}$, and so $\\vec{v}^T S^T = 0 \\implies S\\vec{v} = 0$. Therefore, the vectors $\\vec{v}$ all lie within the $4-2 = 2$-dimensional **nullspace** of $S$. \n",
    "\n",
    "Although you were **not required to do so,** we can find a basis for such $\\vec{v}$ by finding the nullspace of $S$:\n",
    "\\begin{align}\n",
    "S = \\begin{pmatrix} -1 & 1 & 0 & 2 \\\\ 1 & -2 & -4 & 1 \\\\ -1 & 0 & -4 & 5 \\end{pmatrix}\n",
    "\\rightarrow \\begin{pmatrix} -1 & 1 & 0 & 2 \\\\ 0 & -1 & -4 & 3 \\\\  0 & -1 & -4 & 3  \\end{pmatrix}\n",
    "\\rightarrow \\begin{pmatrix} 1 & -1 & 0 & -2 \\\\ 0 & 1 & 4 & -3 \\\\  0 & 0 & 0 & 0  \\end{pmatrix}\n",
    "\\rightarrow \\begin{pmatrix} 1 & 0 & 4 & -5 \\\\ 0 & 1 & 4 & -3 \\\\  0 & 0 & 0 & 0  \\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "We can then calculate the special solutions of $Sx = 0$, which are \n",
    "\\begin{align}\n",
    "\\begin{pmatrix} -4 \\\\ -4 \\\\ 1 \\\\ 0 \\end{pmatrix} \\;\\; \\text{and} \\;\\; \\begin{pmatrix} 5 \\\\ 3 \\\\ 0 \\\\ 1 \\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "We conclude that $\\frac{d(\\vec{v}^T \\vec{x})}{dt} = 0$ for $\\vec{v}$ of the form:\n",
    "\\begin{align}\n",
    "\\vec{v} = c_1 \\begin{pmatrix} -4 \\\\ -4 \\\\ 1 \\\\ 0 \\end{pmatrix} + c_2\\begin{pmatrix} 5 \\\\ 3 \\\\ 0 \\\\ 1 \\end{pmatrix}\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.3",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
